/* Include benchmark-specific header. */
#include "gemver.h"
#define max(x, y) ( (x) > (y) ? (x) : (y) )
#define min(x, y) ( (x) < (y) ? (x) : (y) )

// служебные переменные, опции для распределения данных
double bench_t_start, bench_t_end;
// количество используемых для рассчета процессов при запуске программы
int use_proc;

// зададим массив с отображениями работающих в текущий момент пороцессов
int active_proc[200];
// опциональный массив запасных процессов
int spare_proc[200];

// здадим флаг определяюший будет ли убит процесс
int error_flag = 1;

// вычисление времени работы программы
static
double rtclock()
{
    struct timeval Tp;
    int stat;
    stat = gettimeofday (&Tp, NULL);
    if (stat != 0)
        printf ("Error return from gettimeofday: %d", stat);
    return (Tp.tv_sec + Tp.tv_usec * 1.0e-6);
}

// функции установки таймера
void bench_timer_start()
{
    bench_t_start = rtclock ();
}

// функции установки таймера
void bench_timer_stop()
{
    bench_t_end = rtclock ();
}

// функции установки таймера
void bench_timer_print()
{
    printf ("%0.6lf\n", bench_t_end - bench_t_start);
}

// функция вывода массива для отладки
static
void print_array(int n,
    double w[ n])
{
    int i;
    fprintf(stderr, "==BEGIN DUMP_ARRAYS==\n");
    fprintf(stderr, "begin dump: %s", "w");
    for (i = 0; i < n; i++) {
        if (i % 20 == 0) fprintf (stderr, "\n");
        fprintf (stderr, "%0.2lf ", w[i]);
    }
    fprintf(stderr, "\nend   dump: %s\n", "w");
    fprintf(stderr, "==END   DUMP_ARRAYS==\n");
}

// основная вычислительная функция
static
void kernel_gemver(int group, // группа определяющая область вычисления процесса
    int n, // общий размер данных
    int size, // часть данных в этом процессе
    double alpha, // параметры для текущей задачи
    double beta, // параметры для текущей задачи
    double A[ size][n], // параметры для текущей задачи в заданных интервалах
    double u1[ size], // параметры для текущей задачи в заданных интервалах
    double v1[ n], // параметры для текущей задачи в заданных интервалах
    double u2[ size], // параметры для текущей задачи в заданных интервалах
    double v2[ n], // параметры для текущей задачи в заданных интервалах
    double w[ size], // параметры для текущей задачи в заданных интервалах
    double x[ n], // параметры для текущей задачи в заданных интервалах
    double y[ size], // параметры для текущей задачи в заданных интервалах
    double z[ n]); // параметры для текущей задачи в заданных интервалах

// функция для инициализации массивов в локале каждого процесса
static
void init_array (int n, // общий размер данных
    int size, // часть данных в этом процессе
    int start, // переменные локали для определения места в данных для процесса
    int end, // переменные локали для определения места в данных для процесса
    double *alpha, // параметры для текущей задачи
    double *beta, // параметры для текущей задачи
    double A[ size][n], // параметры для текущей задачи в заданных интервалах
    double u1[ size], // параметры для текущей задачи в заданных интервалах
    double v1[ n], // параметры для текущей задачи в заданных интервалах
    double u2[ size], // параметры для текущей задачи в заданных интервалах
    double v2[ n], // параметры для текущей задачи в заданных интервалах
    double w[ size], // параметры для текущей задачи в заданных интервалах
    double x[ n], // параметры для текущей задачи в заданных интервалах
    double y[ size], // параметры для текущей задачи в заданных интервалах
    double z[ n]); // параметры для текущей задачи в заданных интервалах

// функция для создания контрольной точки
static
void make_kt(int group, // номер группы исполнения основного алгоритма
    int n, // общий размер данных
    int size, // часть данных в этом процессе
    double alpha, // параметры для текущей задачи
    double beta, // параметры для текущей задачи
    double A[ size][n], // параметры для текущей задачи в заданных интервалах
    double u1[ size], // параметры для текущей задачи в заданных интервалах
    double v1[ n], // параметры для текущей задачи в заданных интервалах
    double u2[ size], // параметры для текущей задачи в заданных интервалах
    double v2[ n], // параметры для текущей задачи в заданных интервалах
    double w[ size], // параметры для текущей задачи в заданных интервалах
    double x[ n], // параметры для текущей задачи в заданных интервалах
    double y[ size], // параметры для текущей задачи в заданных интервалах
    double z[ n]); // параметры для текущей задачи в заданных интервалах

// функция в которой запасной процесс ждет вызова для работы либо уведомления
// о том, что в его услугах при данном запуске не нуждаются
static
void start_after_error(int k, // параметр определяющий распределение данных
                       int m); // параметр определяющий распределение данных


// основноая функция
int main(int argc, char** argv)
{

    /* ---------------------------- MPI ---------------------------- */

    // определим внутренние параметры алгоритма
    int n = N;
    int myrank, ranksize;

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);
    MPI_Comm_size(MPI_COMM_WORLD, &ranksize);

    // зададим окружение для работы с ошибками (мертвыми процессами)
    char errstr[MPI_MAX_ERROR_STRING];
    MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);

    // у нас есть один запасной процесс поэтому use_proc определяем как
    // значение параметра окружения - 1
    use_proc = ranksize - 1;
    int k = n / use_proc;
    int m = n % use_proc;


    // зададим параметры отображения для рабочих процессов, используется
    // исключительно в главном процессе
    for (int i = 0; i < use_proc; i++) {
        active_proc[i] = i;
    }

    // проработает то, что будем делать при сбое
    if (myrank == use_proc) {
        start_after_error(k, m);
    }

    // освновной виток выполнения расчетов
    for (int i = 0; i < use_proc; i++) {
        if (myrank == i) {
            // вычислим как будем распределять данные по процессам
            int start = min(i, m) + i * k;
            int size = k + (i < m);
            int end = start + size;

            // зададим служебные переменные заглушки
            MPI_Request req[1];
            MPI_Status status[1];

            // выделим память под данные в локали каждого процесса
            double alpha;
            double beta;
            double (*A)[size][n]; A = (double(*)[size][n])malloc ((size) * (n) * sizeof(double));
            double (*u1)[size]; u1 = (double(*)[size])malloc ((size) * sizeof(double));
            double (*v1)[n]; v1 = (double(*)[n])malloc ((n) * sizeof(double));
            double (*u2)[size]; u2 = (double(*)[size])malloc ((size) * sizeof(double));
            double (*v2)[n]; v2 = (double(*)[n])malloc ((n) * sizeof(double));
            double (*w)[size]; w = (double(*)[size])malloc ((size) * sizeof(double));
            double (*x)[n]; x = (double(*)[n])malloc ((n) * sizeof(double));
            double (*y)[size]; y = (double(*)[size])malloc ((size) * sizeof(double));
            double (*z)[n]; z = (double(*)[n])malloc ((n) * sizeof(double));

            // инициализируем часть массива для текущего процесса
            init_array (n, size, start, end, &alpha, &beta,
                       *A,
                       *u1,
                       *v1,
                       *u2,
                       *v2,
                       *w,
                       *x,
                       *y,
                       *z);

            // начнем отсчет времени с начального момента вычисления
            double time_1;
            if (myrank == 0) {
                time_1 = MPI_Wtime();
            }

            // все рабочие процессы заходят в область вычисления
            kernel_gemver (0, n, size, alpha, beta,
                           *A,
                           *u1,
                           *v1,
                           *u2,
                           *v2,
                           *w,
                           *x,
                           *y,
                           *z);

            // после того как все процессы выполнили свою часть работы, нам
            // необходимо собрать информацию в главном процессе, но на данном
            // этапе мы можем получить информацию, что один из процессов умер,
            // осуществим востановление
            if (myrank == 0) {
                // главный процесс должен собрать всю информацию

                // определим распределение данных
                size = k + (0 < m);

                // выведим свою часть данных
                for (int k = 0; k < size; k++) {
                    printf("%lf\n", (*w)[k]);
                }

                // выделим память под данные, которые будем получать при обмене
                double (*cur)[size];
                cur = (double(*)[size])malloc ((size) * sizeof(double));

                // осуществим синхронизацию между процессами
                for (int j = 1; j < use_proc; j++) {
                    // определим распределение данных
                    size = k + (j < m);
                    printf("Wait message from %d\n", active_proc[j]);
                    // определим статус получения информации
                    int rc = MPI_Recv(*cur, size, MPI_DOUBLE, active_proc[j],
                                      13, MPI_COMM_WORLD, &status[0]);

                    // мы смогли получить данные от процесса?
                    if (rc == 0) {
                        // данные получены, все процессы в работе
                        printf("We recive message from %d\n", active_proc[j]);
                    } else {
                        // данные получены не были, в процессе произошел сбой
                        printf("Process %d was died\n", active_proc[j]);

                        // отправим сообщение резервному процессу, чтобы он
                        // немедленно начал работу
                        MPI_Send(&active_proc[j], 1, MPI_INT, use_proc,
                                 13, MPI_COMM_WORLD);

                        // переопределим отобрежение, и на место сбойного
                        // процесса поставим запасной процесс
                        active_proc[j] = use_proc;

                        // вернемся на шаг назад, где мы сможем получить
                        // информацию от процесса, который перевыполнил работу
                        // сбойного процесса
                        j--;
                        continue;
                    }

                    // выведем полученные данные
                    for (int k = 0; k < size; k++) {
                        printf("%lf\n", (*cur)[k]);
                    }
                }

                // если сбоев не произошло, то в конце работы программы, нам
                // нужно сообщить запасному процессу, что сбоев не было,
                // для этого отправим ему число 0, и при его получении он
                // завершит работу
                int for_send = 0;
                MPI_Send(&for_send, 1, MPI_INT, use_proc, 13, MPI_COMM_WORLD);

                // завершим подсчет времени, и выведем затраченно на расчеты
                // время
                printf("MPI --- %lf\n", MPI_Wtime() - time_1);
            } else {

                // если процесс не главный, то он дожен отправить свою часть
                // вычисленных данных главному процессу
                MPI_Send(w, size, MPI_DOUBLE, 0, 13, MPI_COMM_WORLD);
            }

            // очистим память, которую выделили под векторные данные
            free((void*)A);
            free((void*)u1);
            free((void*)v1);
            free((void*)u2);
            free((void*)v2);
            free((void*)w);
            free((void*)x);
            free((void*)y);
            free((void*)z);
            break;
        }
    }

    // завершим работу всех процессов
    MPI_Finalize();

    /* ---------------------------- MPI ---------------------------- */
    return 0;
}

// функция для инициализации массивов в локали каждого процесса
static
void init_array (int n, // общий размер данных
    int size, // часть данных в этом процессе
    int start, // переменные локали для определения места в данных для процесса
    int end, // переменные локали для определения места в данных для процесса
    double *alpha, // параметры для текущей задачи
    double *beta, // параметры для текущей задачи
    double A[ size][n], // параметры для текущей задачи в заданных интервалах
    double u1[ size], // параметры для текущей задачи в заданных интервалах
    double v1[ n], // параметры для текущей задачи в заданных интервалах
    double u2[ size], // параметры для текущей задачи в заданных интервалах
    double v2[ n], // параметры для текущей задачи в заданных интервалах
    double w[ size], // параметры для текущей задачи в заданных интервалах
    double x[ n], // параметры для текущей задачи в заданных интервалах
    double y[ size], // параметры для текущей задачи в заданных интервалах
    double z[ n]) // параметры для текущей задачи в заданных интервалах
{
    // инициализация полностью определена параметрами задачи
    *alpha = 1.5;
    *beta = 1.2;

    // инициализация полностью определена параметрами задачи
    double fn = (double)n;
    for (int i = 0; i < n; i++) {
        v1[i] = ((i+1)/fn)/4.0;
        v2[i] = ((i+1)/fn)/6.0;
        z[i] = ((i+1)/fn)/9.0;
        x[i] = 0.0;
    }

    // инициализация полностью определена параметрами задачи
    for (int i = start; i < end; i++) {
        u1[i - start] = i;
        u2[i - start] = ((i+1)/fn)/2.0;
        y[i - start] = ((i+1)/fn)/8.0;
        w[i- start] = 0.0;
        for (int j = 0; j < n; j++) {
            A[i - start][j] = (double) (i*j % n) / n;
        }
    }
}

// функция для создания контрольной точки
static
void make_kt(int group, // номер группы исполнения основного алгоритма
    int n, // общий размер данных
    int size, // часть данных в этом процессе
    double alpha, // параметры для текущей задачи
    double beta, // параметры для текущей задачи
    double A[ size][n], // параметры для текущей задачи в заданных интервалах
    double u1[ size], // параметры для текущей задачи в заданных интервалах
    double v1[ n], // параметры для текущей задачи в заданных интервалах
    double u2[ size], // параметры для текущей задачи в заданных интервалах
    double v2[ n], // параметры для текущей задачи в заданных интервалах
    double w[ size], // параметры для текущей задачи в заданных интервалах
    double x[ n], // параметры для текущей задачи в заданных интервалах
    double y[ size], // параметры для текущей задачи в заданных интервалах
    double z[ n]) // параметры для текущей задачи в заданных интервалах
{
    // узнаем номер процесса попавшего на создание контрольной точки
    int myrank;
    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);

    // выделим память под имя файлы контрольной точки текущего процесса
    char name[20];
    snprintf(name, sizeof name, "proc_%d_KT.txt", myrank);
    FILE *fp = fopen(name, "w");

    // контрольные точки создаются асинхронно, о сбое можно узнать на встроенных
    // в программу моментах синхронизации

    // запишем основные атомарные значения, хранящиеся в процессах, в файл
    fprintf(fp, "%d\n", group);
    fprintf(fp, "%d\n", n);
    fprintf(fp, "%d\n", size);
    fprintf(fp, "%.20lf\n", alpha);
    fprintf(fp, "%.20lf\n", beta);


    // будем сохранять данные с большой точностью, чтобы получить точный ответ

    // запишем векторые данные локали процесса в файл
    for (int i = 0; i < size; i++) {
        for (int j = 0; j < n; j++) {
            fprintf(fp, "%.20lf ", A[i][j]);
        }
        fprintf(fp, "\n");
    }

    // запишем векторые данные локали процесса в файл
    for (int i = 0; i < size; i++) {
        fprintf(fp, "%.20lf ", u1[i]);
    }
    fprintf(fp, "\n");

    // запишем векторые данные локали процесса в файл
    for (int i = 0; i < n; i++) {
        fprintf(fp, "%.20lf ", v1[i]);
    }
    fprintf(fp, "\n");

    // запишем векторые данные локали процесса в файл
    for (int i = 0; i < size; i++) {
        fprintf(fp, "%.20lf ", u2[i]);
    }
    fprintf(fp, "\n");

    // запишем векторые данные локали процесса в файл
    for (int i = 0; i < n; i++) {
        fprintf(fp, "%.20lf ", v2[i]);
    }
    fprintf(fp, "\n");

    // запишем векторые данные локали процесса в файл
    for (int i = 0; i < size; i++) {
        fprintf(fp, "%.20lf ", w[i]);
    }
    fprintf(fp, "\n");

    // запишем векторые данные локали процесса в файл
    for (int i = 0; i < n; i++) {
        fprintf(fp, "%.20lf ", x[i]);
    }
    fprintf(fp, "\n");

    // запишем векторые данные локали процесса в файл
    for (int i = 0; i < size; i++) {
        fprintf(fp, "%.20lf ",y[i]);
    }
    fprintf(fp, "\n");

    // запишем векторые данные локали процесса в файл
    for (int i = 0; i < n; i++) {
        fprintf(fp, "%.20lf ", z[i]);
    }
    fprintf(fp, "\n");

    fclose(fp);
}

// функция в которой запасной процесс ждет вызова для работы либо уведомления
// о том, что в его услугах при данном запуске не нуждаются
static
void start_after_error(int k, // параметр определяющий распределение данных
                       int m) // параметр определяющий распределение данных
{
    // зададим переменные заглушки для служебной информации
    MPI_Status status[1];
    int cur;

    // в данную функцию попадает только запасной процесс, он попадает на эту
    // точку и ожидает получения информации от главного процесса
    // если получено сообщение отличное от 0, то это означает, что запасной
    // процесс должен выполнять работу за процесс с полученным номером немедленно,
    // если получено значение 0, значит сбоя в работе программы не произошло и
    // нужно завершить работу текущего процесса
    MPI_Recv(&cur, 1, MPI_INT, 0, 13, MPI_COMM_WORLD, &status[0]);

    // произошла ошибка в процессе с номером cur?
    if (cur != 0) {
        // произошла ошибка
        printf("Error caused\n");
        fflush(stdout);

        // востановим данные из контрольной точки процесса с номером cur
        char name[20];
        snprintf(name, sizeof name, "proc_%d_KT.txt", cur);
        FILE *fp = fopen(name, "r");

        // определим место, с которого нам нужно продолжить вычисления
        int group;
        int n;
        int size;
        double alpha;
        double beta;

        // считаем атомарные данные
        fscanf(fp, "%d", &group);
        fscanf(fp, "%d", &n);
        fscanf(fp, "%d", &size);
        fscanf(fp, "%lf", &alpha);
        fscanf(fp, "%lf", &beta);

        // выделим память и считаем векторные данные
        double (*A)[size][n];
        A = (double(*)[size][n])malloc ((size) * (n) * sizeof(double));
        for (int i = 0; i < size; i++) {
            for (int j = 0; j < n; j++) {
                fscanf(fp, "%lf", &((*A)[i][j]));
            }
        }

        // выделим память и считаем векторные данные
        double (*u1)[size];
        u1 = (double(*)[size])malloc ((size) * sizeof(double));
        for (int i = 0; i < size; i++) {
            fscanf(fp, "%lf", &((*u1)[i]));
        }

        // выделим память и считаем векторные данные
        double (*v1)[n];
        v1 = (double(*)[n])malloc ((n) * sizeof(double));
        for (int i = 0; i < n; i++) {
            fscanf(fp, "%lf", &((*v1)[i]));
        }

        // выделим память и считаем векторные данные
        double (*u2)[size];
        u2 = (double(*)[size])malloc ((size) * sizeof(double));
        for (int i = 0; i < size; i++) {
            fscanf(fp, "%lf", &((*u2)[i]));
        }

        // выделим память и считаем векторные данные
        double (*v2)[n];
        v2 = (double(*)[n])malloc ((n) * sizeof(double));
        for (int i = 0; i < n; i++) {
            fscanf(fp, "%lf", &((*v2)[i]));
        }

        // выделим память и считаем векторные данные
        double (*w)[size];
        w = (double(*)[size])malloc ((size) * sizeof(double));
        for (int i = 0; i < size; i++) {
            fscanf(fp, "%lf", &((*w)[i]));
        }

        // выделим память и считаем векторные данные
        double (*x)[n];
        x = (double(*)[n])malloc ((n) * sizeof(double));
        for (int i = 0; i < n; i++) {
            fscanf(fp, "%lf", &((*x)[i]));
        }

        // выделим память и считаем векторные данные
        double (*y)[size];
        y = (double(*)[size])malloc ((size) * sizeof(double));
        for (int i = 0; i < size; i++) {
            fscanf(fp, "%lf", &((*y)[i]));
        }

        // выделим память и считаем векторные данные
        double (*z)[n];
        z = (double(*)[n])malloc ((n) * sizeof(double));
        for (int i = 0; i < n; i++) {
            fscanf(fp, "%lf", &((*z)[i]));
        }

        // мы успешно востановили данные из контрольной точки умершего процесса,
        // или процесса в котором произошел сбой
        // далее нам необходимо отрпавить наш запасной процесс в ту точку
        // программы до которой у нас есть вся информации о вычислениях,
        // полученная из контрольной точки, эта информация сохранена в переменной
        // group, продолжаем вычисления
        kernel_gemver (group, n, size, alpha, beta,
                       *A,
                       *u1,
                       *v1,
                       *u2,
                       *v2,
                       *w,
                       *x,
                       *y,
                       *z);

        // после вычислений нам необходимо отправить главному процессу информацию,
        // которую мы вычислели

        // определим распределение данных
        size = k + (0 < m);

        // отправим нашу вычисленную часть ответа
        MPI_Send(w, size, MPI_DOUBLE, 0, 13, MPI_COMM_WORLD);

        // очистим память, которую выделили под векторные данные
        free((void*)A);
        free((void*)u1);
        free((void*)v1);
        free((void*)u2);
        free((void*)v2);
        free((void*)w);
        free((void*)x);
        free((void*)y);
        free((void*)z);

    } else {
        // ошибки не произошло, значит нам не нужно выполнять работу
        printf("Error doesn't caused\n");
        fflush(stdout);
    }
}

// основная вычислительная функция
static
void kernel_gemver(int group, // группа определяющая область вычисления процесса
    int n, // общий размер данных
    int size, // часть данных в этом процессе
    double alpha, // параметры для текущей задачи
    double beta, // параметры для текущей задачи
    double A[ size][n], // параметры для текущей задачи в заданных интервалах
    double u1[ size], // параметры для текущей задачи в заданных интервалах
    double v1[ n], // параметры для текущей задачи в заданных интервалах
    double u2[ size], // параметры для текущей задачи в заданных интервалах
    double v2[ n], // параметры для текущей задачи в заданных интервалах
    double w[ size], // параметры для текущей задачи в заданных интервалах
    double x[ n], // параметры для текущей задачи в заданных интервалах
    double y[ size], // параметры для текущей задачи в заданных интервалах
    double z[ n]) // параметры для текущей задачи в заданных интервалах
{

    // определим номер процесса, которые попал в эту точку программы
    int myrank;
    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);

    // в зависимости от целевой группы задач процесса, перейдем к нужной части
    // программы
    if (group <= 1) {
        // block 1

        // создадим контрольную точку
        make_kt (1, n, size, alpha, beta,
                 A,
                 u1,
                 v1,
                 u2,
                 v2,
                 w,
                 x,
                 y,
                 z);

        // произведем вычисления для первой целевой группы
        for (int i = 0; i < size; i++) {

            // синтетически убьем процесс (последний из группы)
            if (myrank == (use_proc - 1) && i == 20 && error_flag) {
                printf("Rank %d / %d: bye bye!\n", myrank, use_proc - 1);
                raise(SIGKILL);
            }
            for (int j = 0; j < n; j++) {
                A[i][j] = A[i][j] + u1[i] * v1[j] + u2[i] * v2[j];
            }
        }
    }

    // в зависимости от целевой группы задач процесса, перейдем к нужной части
    // программы
    if (group <= 2) {
        //block 2

        // создадим контрольную точку
        make_kt (2, n, size, alpha, beta,
                 A,
                 u1,
                 v1,
                 u2,
                 v2,
                 w,
                 x,
                 y,
                 z);

        // произведем вычисления для первой целевой группы
        for (int i = 0; i < size; ++i) {
            for (int j = 0; j < n; ++j) {
                x[j] = x[j] + beta * A[i][j] * y[i];
            }
        }

        /* ---------------------------- MPI ---------------------------- */

        // в данной точке вычислений процессам необходимо обменяться информацией
        // создадим служебные переменные заглушки
        MPI_Status status[1];
        // выделим память под массивы для обмена данными
        double (*cur)[n]; cur = (double(*)[n])malloc ((n) * sizeof(double));

        // нулевой процесс является главным в нашем устройстве мира вычислений
        if (!myrank) {
            // досчитаем нашу часть массива х
            for (int j = 0; j < n; j++) {
                x[j] = x[j] + z[j];
            }

            // получим информацию от всех работающих процессов
            for (int i = 1; i < use_proc; i++) {

                // используем матрицу отображения для общения с рабочими процессами
                printf("Wait message from %d\n", active_proc[i]);

                // получим информацию о статусе получения информации
                int rc = MPI_Recv((*cur), n, MPI_DOUBLE, active_proc[i],
                                  13, MPI_COMM_WORLD, &status[0]);

                // получили ли мы информацию от процесса с заданным номером
                if (rc == 0) {
                    // информация получена успешно
                    printf("We recive message from %d\n", active_proc[i]);
                } else {
                    // информацию плоучить не удалось, процесс мертв
                    printf("Process %d was died\n", active_proc[i]);
                    // отправим сообщение запасному процессу, чтобы он приступил
                    // к работе
                    MPI_Send(&active_proc[i], 1, MPI_INT,
                             use_proc, 13, MPI_COMM_WORLD);

                    // зададим отображение на то, что сломанный процесс вышел из
                    // строя, заменим его на запасной процесс, который приступит
                    // к работе после получения сообщения об ошибке в процессе
                    active_proc[i] = use_proc;

                    // перейдем на шаг назад, чтобы снова совершить обмен данными,
                    // там запасной процесс снова отправит информацию, после того
                    // как пересчитает ее
                    i--;
                    continue;
                }

                // сформиурем в главном процессе всю информацию от служеных
                // процессов
                for (int j = 0; j < n; j++) {
                    x[j] = x[j] + (*cur)[j];
                }
            }

            // отправим информацию, которую мы согрегировали в главном процессе
            // во все служебные процессы, которые работают в данный момент,
            // поэтому мы используем массив отображений
            for (int i = 1; i < use_proc; i++) {
                MPI_Send(x, n, MPI_DOUBLE, active_proc[i], 13, MPI_COMM_WORLD);
            }
        } else {
            // не главный процесс должен оставить свою часть данных главному
            // процессу
            MPI_Send(x, n, MPI_DOUBLE, 0, 13, MPI_COMM_WORLD);

            // примем информацию после агрегации в главном процессе
            MPI_Recv(x, n, MPI_DOUBLE, 0, 13, MPI_COMM_WORLD, &status[0]);
        }

        /* ---------------------------- MPI ---------------------------- */
    }

    // в зависимости от целевой группы задач процесса, перейдем к нужной части
    // программы
    if (group <= 3) {
        //block3

        // создадим котрольную точку
        make_kt (3, n, size, alpha, beta,
                 A,
                 u1,
                 v1,
                 u2,
                 v2,
                 w,
                 x,
                 y,
                 z);

        // произведем дальнейшие вычислений в нашей группе
        for (int i = 0; i < size; i++) {
            for (int j = 0; j < n; j++) {
                w[i] = w[i] + alpha * A[i][j] * x[j];
            }
        }
    }
}
